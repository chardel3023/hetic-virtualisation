Volume horaire FFP (face à face pédagogique) 7

Contenu FFP
	

Historique de la Virtualisation (UC et Stockage)
La virtualisation des réseaux (Services réseaux virtualisés - VPN)
La virtualisation des serveurs
La virtualisation du stockage
Concept de la virtualisation du poste de travail
Concept de la virtualisation applicative
Travaux pratiques sur l'installation et la configuration d'un environnement de virtualisation (Virtualbox, Esx, ...)

Objectifs pédagogiques
	

• Maitrise des fondamentaux de la virtualisation
• Connaissance de l'architecture d'un hyperviseur

Savoirs associés
	

• Historique de la Virtualisation (UC et Stockage)
• La virtualisation des réseaux (Services réseaux virtualisés - VPN)


Virtualisation de réseau
Qu’est-ce que la virtualisation de réseau (NV) ?

 

La virtualisation de réseau (NV) désigne l’abstraction sous forme logicielle des ressources réseau traditionnellement fournies sous forme matérielle. La virtualisation de réseau peut combiner plusieurs réseaux physiques en un réseau logiciel virtuel, ou encore diviser un réseau physique en plusieurs réseaux virtuels indépendants et distincts.

 
Pourquoi la virtualisation de réseau ?

La virtualisation de réseau découple les services réseau du matériel sous-jacent et permet d’effectuer le provisionnement virtuel de tout un réseau. Les ressources du réseau physique, telles que les commutateurs et les routeurs, sont regroupées et accessibles par n’importe quel utilisateur par l’intermédiaire d’un système de gestion centralisé. La virtualisation de réseau permet également de procéder à l’automatisation de nombre de tâches administratives, ce qui réduit ainsi le nombre d’erreurs manuelles et le délai de provisionnement. Elle renforce la productivité et l’efficacité du réseau.
virtualisation de réseau vmware pour les nuls
Rubriques connexes

    Virtualisation des fonctions réseau (Network Functions Virtualization, NFV)
    Micro-segmentation
    Virtualisation des serveurs

Exemple de virtualisation de réseau

Les LAN virtuels (VLAN) sont un exemple de virtualisation de réseau. Un VLAN est une sous-section d’un réseau local (LAN) créée avec un logiciel qui combine les terminaux du réseau en un groupe, quel que soit leur emplacement physique. Les VLAN améliorent la vitesse et les performances des réseaux surchargés et simplifient les opérations de modification ou d’ajout effectuées sur le réseau.

 
Types de virtualisation de réseau

La virtualisation de réseau peut être de type externe ou interne. La virtualisation externe combine plusieurs réseaux ou parties de réseau en une unité virtuelle. La virtualisation interne utilise des conteneurs logiciels pour reproduire ou fournir la fonctionnalité d’un réseau physique unique.

 
Virtualisation de réseau NSX

VMware NSX Data Center est une plate-forme de virtualisation de réseau qui fournit l’intégralité des fonctions réseau et de sécurité sous forme logicielle et qui est isolée de l’infrastructure physique sous-jacente. NSX utilise des logiciels pour fournir des fonctions réseau telles que les pare-feu, la commutation et le routage. Autrement dit, plusieurs utilisateurs peuvent partager le même environnement physique en utilisant des réseaux virtuels invisibles les uns pour les autres pour accroître l’efficacité et la sécurité.

 
Logiciel de virtualisation de réseau

Un logiciel de virtualisation de réseau permet aux administrateurs réseau de déplacer des machines virtuelles à l’échelle de différents domaines sans avoir à reconfigurer le réseau. Le logiciel crée une superposition de réseau qui peut exécuter des couches de réseau virtuel séparées par-dessus un même réseau physique.

• La virtualisation des serveurs
• La virtualisation du stockage


Qu’est-ce que la virtualisation du stockage et à quoi sert-elle ?

Dans le monde du stockage d’entreprise, la virtualisation du stockage est une technique qui permet de mettre en commun des dispositifs de stockage physiques de manière à ce que le département informatique n’ait qu’un seul dispositif de stockage « virtuel » à gérer. Bien qu’aujourd’hui largement éclipsée par le modèle cloud, la virtualisation du stockage offrait des avantages opérationnels et économiques considérables par rapport au stockage bare metal (dans lequel les dispositifs de stockage physiques sont exploités directement). La virtualisation du stockage permet aux entreprises de réduire les problèmes de compatibilité ainsi que d’améliorer les performances et la sécurité des environnements de stockage.
En utilisant le stockage virtualisé, les administrateurs ont dû tenir compte d’un certain nombre de problèmes :

    Compatibilité et interopérabilité – Les environnements de virtualisation du stockage doivent être compatibles avec l’infrastructure matérielle sous-jacente, les composants réseau, les serveurs, les systèmes d’exploitation, les outils de gestion et les hyperviseurs. Il est essentiel de garantir une compatibilité avec des protocoles comme NFS et Fibre Channel pour assurer une intégration fluide, ainsi qu’avec les API offrant l’interopérabilité avec les outils d’automatisation qui assurent l’orchestration et l’intégration. 
    Performances et latence – Les applications critiques ont des exigences de performances qui doivent être satisfaites par l’environnement de stockage virtualisé. Les administrateurs doivent évaluer les capacités des contrôleurs de stockage, la bande passante du réseau, la capacité d’I/O des disques et les mécanismes de mise en cache pour comprendre leur effet sur les performances et la latence.
    Sécurité et intégrité des données – Un environnement de stockage virtualisé doit prendre en charge le chiffrement des données afin de garantir leur sécurité pendant le transit ou au repos et de fournir des contrôles d’accès et d’authentification à des niveaux granulaires pour prévenir le piratage. De robustes solutions de sauvegarde et de reprise après sinistre sont nécessaires pour assurer la protection des données dans le cadre du stockage virtualisé.

Solutions, produits ou services HPE connexes

Infrastructure hyperconvergée

Stockage pour la virtualisation de serveurs
Sujets connexes

Virtualisation de serveurs

Bureaux virtuels

Virtualisation
Comment la virtualisation du stockage s’inscrit-elle dans le modèle de stockage cloud ?

La technique de virtualisation du stockage existe depuis des décennies et a ouvert la voie au stockage cloud (qui permet de virtualiser des quantités massives de dispositifs de stockage individuels en d’énormes pools de stockage virtuel). Ces dernières années, la virtualisation du stockage a largement cédé la place au modèle de stockage cloud, les départements informatiques des entreprises ayant compris qu’il était dans leur intérêt de laisser les fournisseurs de services cloud résoudre les problématiques de déploiement et de gestion du stockage virtualisé mentionnées plus haut.

Avec le modèle du cloud computing, les administrateurs de machines virtuelles et d’applications n’ont qu’à renseigner quelques paramètres pour spécifier le stockage dont ils ont besoin – type de stockage (bloc, fichier, objet), capacité, performances et bande passante –, et le fournisseur de services cloud met immédiatement à disposition le stockage souhaité à partir de ses propres pools virtualisés. En arrière-plan, l’architecture cloud prend en charge toutes les tâches de provisionnement, de configuration et de protection qui étaient auparavant réalisées en datacenter par un administrateur.

Les avantages opérationnels du modèle cloud sont évidents : les services informatiques de l’entreprise n’ont plus à se préoccuper de la gestion d’environnements de stockage virtualisés de plus en plus complexes, et les différentes divisions profitent de délais de provisionnement nettement raccourcis. Le stockage cloud permet un accès à la demande aux ressources de stockage et simplifie considérablement les processus de gestion des données. Cela permet d’accélérer la création de valeur et la transformation digitale.

D’un point de vue financier, le modèle cloud offre aux entreprises un choix et une flexibilité accrus en matière de consommation des ressources de stockage. En règle générale, les fournisseurs de services cloud proposent une facturation de type OpEx en paiement à l’utilisation pour la consommation de stockage cloud. Cependant, les offres les plus avancées intègrent depuis peu une option de type CapEx, qui permet aux entreprises de combiner la souveraineté des données et les avantages sécuritaires du stockage sur site avec une expérience opérationnelle cloud.
Comment fonctionne la virtualisation du stockage ?

Avec la virtualisation du stockage,le matériel de stockage physique est mis en miroir dans un volume virtuel. Pour construire un environnement de stockage virtuel simple, plusieurs disques physiques sont combinés en un groupe sur un seul serveur. Les blocs de stockage virtuels ou logiques sont affectés au même serveur et permettent de rediriger le trafic d’entrée/sortie (I/O).

Les disques physiques sont séparés du volume virtuel par une couche de virtualisation qui permet aux systèmes d’exploitation et aux applications d’accéder au stockage et de l’utiliser. Le logiciel de stockage virtuel prend en charge les demandes d’I/O et les envoie aux dispositifs de stockage appropriés dans le pool global de stockage.

Les disques physiques sont eux-mêmes divisés en petits blocs de données, ou objets appelés numéros d’unités logiques (LUN), volumes logiques ou groupes RAID. Ces blocs sont présentés aux serveurs à distance sous la forme d’un disque virtuel. Du point de vue du serveur, ils ressemblent à un disque physique plutôt qu’à l’ensemble des dispositifs de stockage qui constituent le pool global de stockage dans l’environnement virtualisé.

Dans un environnement plus complexe, les baies RAID peuvent fonctionner comme un stockage virtuel. Dans ce cas, plusieurs disques physiques imitent un dispositif de stockage unique qui répartit et réplique les données sur plusieurs disques en arrière-plan. Ce processus améliore les performances d’I/O et protège les données de la défaillance d’un seul disque.

Le mode d’accès aux données des disques physiques implique une étape supplémentaire pour le logiciel de virtualisation. En plus de créer une séparation entre les dispositifs de stockage physiques et virtuels, le logiciel de virtualisation crée une carte à l’aide de métadonnées pour localiser rapidement les données stockées. Dans certains cas, le logiciel crée un algorithme pour trouver les données encore plus rapidement.

La création d’un stockage virtuel peut être réalisée dans des environnements de stockage de bloc aussi bien que de stockage de fichier. La virtualisation d’un réseau de stockage (SAN) implique l’ajout d’une couche de traduction entre les hôtes et les baies de stockage. Dans ce type de virtualisation du stockage, les serveurs sont redirigés vers des LUN virtualisés plutôt que vers des LUN basés sur une baie de stockage donnée. Ces LUN virtualisés restent sur le dispositif virtualisé. La virtualisation d’un NAS consiste à supprimer les dépendances entre les données auxquelles on accède au niveau des fichiers et l’endroit où les fichiers sont physiquement stockés.
Quels sont les différents types de virtualisation du stockage ?

Il existe plusieurs façons d’appliquer le stockage à un environnement virtuel, selon que la virtualisation est basée sur l’hôte, sur les baies ou sur le réseau.
Virtualisation basée sur l’hôte

Utilisée le plus souvent dans les systèmes HCI et le stockage cloud, la virtualisation basée sur l’hôte utilise un logiciel pour diriger le trafic. Dans cette méthode, le stockage physique peut être attribué à pratiquement n’importe quel dispositif ou baie. L’hôte, ou un système hyperconvergé composé de plusieurs hôtes, présente des disques virtuels à des machines invitées possédant n’importe quel type de configuration, par exemple des machines virtuelles dans une entreprise, des ordinateurs accédant à des fichiers partagés ou des serveurs accédant à des données via le cloud.
Virtualisation basée sur les baies

Avec la virtualisation du stockage basée sur des baies physiques, les serveurs sont localisés physiquement, et la baie à laquelle un utilisateur accède n’est pas visible pour les serveurs ou pour les utilisateurs accédant au stockage. Dans ce cas, une baie de stockage sert de contrôleur de stockage principal, et utilise un logiciel de virtualisation pour regrouper les ressources de stockage d’autres baies. La baie peut également présenter différents types de stockage physique comme autant de niveaux, plutôt qu’une collection disparate de dispositifs. Ces niveaux peuvent être constitués de disques SSD ou HDD présents sur les différentes baies.

 

 
Virtualisation basée sur le réseau

La forme la plus courante de virtualisation du stockage est basée sur le réseau. Dans ce cas, tous les dispositifs de stockage sont reliés à un réseau de stockage FC (Fiber Channel) ou ISCSI (Internet Small Computing System Interface) par un dispositif réseau. Ces dispositifs interconnectés se présentent comme un pool virtuel unique au sein de leur réseau de stockage.
Quelle est la différence entre la virtualisation des serveurs et la virtualisation du stockage ?

Les entreprises ont le choix entre plusieurs technologies de virtualisation en fonction de leurs besoins. La plupart des entreprises intègrent désormais des postes de travail virtuels dans leur environnement de travail, car le télétravail semble désormais être là pour durer. La virtualisation des applications fait partie de cet environnement de télétravail, dans la mesure où un poste de travail a besoin de moins de ressources pour remplir sa fonction dès lors que les systèmes d’exploitation des applications ne résident pas physiquement sur celui-ci.

Mais ce n’est pas tout. Un serveur virtualisé garantit une disponibilité plus élevée et une reprise après sinistre plus efficace à l’entreprise. En effet, le système d’exploitation est séparé du matériel, et les machines qui y accèdent peuvent être traitées comme un fichier. En tant que fichiers, ces serveurs peuvent être stockés sur le réseau de stockage, ce qui permet une mobilité accrue pour l’accès aux données. En cas de défaillance d’un serveur, l’activité peut être hébergée sur un autre serveur puisque tous les serveurs virtuels sont stockés sur le réseau SAN.

La virtualisation des serveurs permet également d’adapter le matériel à la demande pour bénéficier d’un système plus élastique. Certains serveurs peuvent être arrêtés lorsque les charges de travail diminuent, puis remis en service dès que les charges de travail augmentent. En réduisant ainsi le nombre de serveurs simultanément actifs, une entreprise peut réduire ses coûts d’électricité et de refroidissement, de même que le nombre de machines physiques restant inutilisées la plupart du temps.

La virtualisation du stockage permet également d’accroître la disponibilité, en particulier lorsqu’elle est utilisée en conjonction avec la virtualisation des serveurs. Le stockage est virtualisé, et n’est donc pas associé à un serveur particulier. Ainsi, le stockage peut être géré à partir de sources multiples et utilisé comme un référentiel unique. En outre, de nombreux serveurs peuvent accéder aux données stockées sur le SAN, ce qui simplifie grandement son utilisation.
Quels sont les avantages de la virtualisation du stockage ?

    Réduction des coûts – Comme le stockage virtuel n’a pas besoin des redondances matérielles propres aux architectures de stockage d’entreprise traditionnelles pour la reprise après sinistre, il y a moins d’appareils et de licences logicielles à acheter. L’entreprise peut ainsi réduire ses coûts en évitant de lourds engagements financiers initiaux.
    Gain de temps – Le stockage virtualisé permet non seulement de réduire les temps d’arrêt (planifiés ou non), mais aussi d’effectuer les mises à niveau plus rapidement et avec beaucoup moins de perturbations.
    Évolutivité – Avec le stockage virtuel, les entreprises n’ont plus besoin de prévoir leurs besoins de stockage à long terme ni de payer d’avance pour toute cette capacité. Au lieu de cela, les services informatiques peuvent tirer parti d’un provisionnement dynamique qui répond à des besoins changeants, à la demande.
    Gestion simplifiée – Le stockage virtuel simplifie et améliore l’utilisation des ressources en offrant la possibilité d’ajouter ou de supprimer facilement des capacités de stockage sans interrompre les applications. De plus, il permet une migration transparente des données et fluidifie la mise en œuvre de fonctions avancées à l’échelle du pool de stockage.
    Risques réduits – En cas de défaillance d’un disque, d’un contrôleur de stockage ou d’un bloc d’alimentation, chacun de ces éléments est déjà mis en miroir dans la baie virtuelle, ce qui réduit considérablement le risque d’interruption. Cette redondance virtuelle limite les ralentissements et augmente l’efficacité et la flexibilité du stockage.
    Gains de productivité – Le stockage virtuel offre non seulement une disponibilité totale : il permet aussi d’accélérer les déploiements d’applications et de services afin d’accélérer le retour sur investissement.
    Gains d’efficacité – Le principal avantage réside dans l’accélération du stockage et dans une meilleure utilisation de la capacité. L’accès aux données et le traitement des données sont également plus stables. En outre, le stockage virtuel garantit une utilisation à 100 % et un faible risque de manque de capacité.

• Concept de la virtualisation du poste de travail


Qu’est-ce qu’un poste de travail virtuel ?

Un poste de travail virtuel est une station de travail qui existe de façon virtuelle et à laquelle il est possible d’accéder depuis n’importe où via Internet. Le poste de travail virtuel possède une image d’un système d’exploitation qui est partagé par d’autres machines virtuelles sur un réseau central.
Les postes de travail virtuels ont le vent en poupe

Avec la généralisation du télétravail et du travail hybride, les entreprises se tournent de plus en plus vers les postes de travail virtuels pour prendre en charge leurs effectifs dispersés. Les services informatiques ont mis en place des environnements de bureau virtuel comptant des dizaines, des centaines ou des milliers de postes de travail virtuels en service.
Solutions, produits ou services HPE connexes

HPE GreenLake pour la VDI

Solutions VDI

Solutions de stockage VDI pour la virtualisation client
La gestion centralisée est un gage de sécurité

Ces postes de travail virtuels reçoivent une image d’un système d’exploitation, ce qui leur permet de fonctionner au moyen d’un système hébergé et partagé ailleurs dans un datacenter. Et comme le datacenter est physiquement hébergé dans un endroit différent et géré de manière centralisée, les postes de travail virtuels sont plus sécurisés qu’un bureau physique, dans lequel de nombreux ordinateurs doivent être gérés séparément.
Rubriques connexes

Virtualisation de serveurs

VDI

Virtualisation

Machine virtuelle (VM)
Quels sont les différents types de postes de travail virtuels ?

La virtualisation des postes de travail peut être réalisée de cinq façons différentes. Chaque méthode ayant ses forces et ses faiblesses, il convient d’étudier soigneusement chaque cas d’utilisation individuel pour garantir une virtualisation réussie.

    Provisionnement du système d’exploitation : Le système d’exploitation est envoyé soit à une machine virtuelle déployée dans le datacenter, soit à une machine réelle sur un poste de travail physique. Dans les deux cas, une connexion constante au datacenter est nécessaire. L’utilisation d’un ordinateur portable n’est donc pas recommandée et les ordinateurs de bureau physiques peuvent nécessiter une importante prise en charge matérielle.
    Protocole Remote Desktop Services (RDS) : La virtualisation est réalisée dans un datacenter, libérant ainsi des ressources pour le client. Dans ce cas, une seule instance d’une application ou d’un système d’exploitation est hébergée sur un serveur partagé. Cela fait du RDS une solution assez économique.
    Hyperviseurs clients : Lorsqu’un client dispose d’un hyperviseur directement sur un ordinateur de bureau, cela permet d’exécuter plusieurs machines virtuelles à la fois. Bien que le matériel local soit généralement plus performant que tout ce qui est hébergé sur un serveur de données, il est peu probable que l’hyperviseur du client dispose du matériel nécessaire pour le prendre en charge. Cela signifie que la machine virtuelle ne fonctionnera probablement pas sur ce matériel.
    Postes de travail virtuels hébergés côté client : Les machines virtuelles sont positionnées de manière à fonctionner au-dessus du système d’exploitation, ce qui permet d’y accéder partout et à tout moment. Cela signifie qu’il y a fondamentalement deux systèmes d’exploitation en jeu ici, ce qui augmente les chances que les machines virtuelles fonctionnent. Cependant, l’utilisation de cette méthode nécessite souvent d’importants efforts de formation et d’acceptation de la part de l’administrateur système.
    Virtualisation des applications : Les applications sont isolées du système d’exploitation de l’utilisateur et fonctionnent de manière totalement indépendante. Cette séparation permet à une variété d’applications de fonctionner en même temps sur la même plateforme sans interférer les unes avec les autres. En utilisant un conteneur pour fournir le « panier » d’applications, les administrateurs peuvent gérer plus efficacement le pool d’applications.

Quels avantages offrent les postes de travail virtuels ?

Intrinsèquement flexibles et conviviaux, les postes de travail virtuels offrent de nombreux avantages.

    Sécurité : Avantage considérable pour les entreprises, chaque dispositif est isolé des données sensibles et des ressources propriétaires, de sorte que ni les unes ni les autres ne sont compromises en cas de perte ou de vol.
    Gestion : Comme les postes de travail virtuels sont gérés de manière centralisée, ils peuvent faire l’objet d’opérations de mise à jour et de maintenance rapides sur tous les sites éloignés.
    Flexibilité : Les administrateurs peuvent rapidement allouer et configurer des postes de travail virtuels, ce qui évite d’avoir à provisionner des dispositifs physiques qui ne seront peut-être pas nécessaires pendant longtemps.
    Coût : Avec des besoins minimes en maintenance et en équipement physique, les postes de travail virtuels offrent un net avantage en termes de coût.
    Puissance de calcul : La puissance réelle qui fait fonctionner les postes de travail virtuels provient du centre de contrôle des données, qui est beaucoup plus puissant que les clients « légers » généralement utilisés.
    Productivité : Certaines études indiquent que le télétravail peut augmenter l’efficacité des employés et donc leur productivité.
    Reprise : Un autre avantage majeur de l’isolation des terminaux par rapport aux données et applications tient au fait que toutes ces ressources sont stockées de manière centralisée et sauvegardées régulièrement.

Quelle est la différence entre un poste de travail virtuel et une machine virtuelle ?

Quelques différences simples distinguent un poste de travail virtuel d’une machine virtuelle.

Tout d’abord, une machine virtuelle (VM) n’est ni plus ni moins qu’un fichier imitant le fonctionnement d’un ordinateur physique. Une VM possède son propre processeur, son propre stockage, sa propre mémoire et sa propre interface réseau, de sorte qu’elle fonctionne comme « un ordinateur dans un ordinateur ». Cependant, une VM a toujours besoin de matériel physique, comme un hyperviseur, un disque dur RAM et une interface réseau. Néanmoins, une VM utilise le système d’exploitation tout comme une machine physique, ce qui fait que l’expérience utilisateur est presque identique. En outre, comme le système d’exploitation reste séparé, une VM peut exécuter simultanément des applications et des processus qui, autrement, interféreraient les uns avec les autres.

Un poste de travail virtuel est créé par des machines virtuelles. Il reproduit la convivialité d’un ordinateur physique sans toutes les structures que possède même une machine virtuelle (processeur, stockage, mémoire, etc.). Il reprend tous les éléments d’un espace de travail physique et les stocke sur un serveur. Ainsi, l’expérience d’utilisation d’un poste de travail virtuel est essentiellement la même que celle d’un ordinateur physique.
Comment les postes de travail virtuels améliorent-ils les processus de travail ?

À l’heure où les entreprises s’empressent de développer leurs espaces de travail virtuels, vous pouvez tirer des enseignements des applications suivantes, quel que soit le secteur d’activité de votre entreprise :

Éducation : Confrontée à une demande d’élargissement de son offre d’enseignement virtuel, l’Universidad Americana au Paraguay a déployé une nouvelle infrastructure composable pour améliorer son agilité informatique. Grâce à ce changement, l’université a pu répondre rapidement aux demandes des étudiants, réduire les coûts informatiques et améliorer l’expérience utilisateur. Le recours à l’automatisation a permis à l’équipe informatique de fournir rapidement des serveurs et des postes de travail virtuels aux étudiants, et de consacrer le temps ainsi libéré à l’innovation plutôt qu’à l’administration informatique.

Construction : Lorsqu’un grand détaillant a décidé de réaménager ses magasins, plutôt que d’envoyer un collaborateur inspecter le travail en cours, il a utilisé la réalité augmentée pour permettre à ce collaborateur de visualiser l’espace physique à distance. La RA a permis aux collaborateurs travaillant sur le projet de se promener dans le magasin afin d’inspecter le site d’un point de vue réaliste, en 3D. Grâce à cela, le détaillant a pu éviter la plupart des déplacements qui auraient autrement été nécessaires pour les multiples inspections.

Télémédecine : L’utilisation de solutions virtuelles permet d’étendre le rayon d’action des équipes soignantes et de procurer aux patients des soins plus sûrs et plus immédiats, au moment et de la manière dont ils en ont besoin. Le fait de travailler dans des environnements virtuels permet au personnel d’assurer la sécurité des dossiers des patients ainsi que de mobiliser et de protéger les fichiers d’imagerie médicale. Un environnement virtuel offre la rapidité et la précision nécessaires pour améliorer les résultats médicaux et les expériences cliniques, et même pour individualiser les soins dans les cas les plus complexes.

Maintenance d’installations : Les postes de travail virtuels sont même adaptés à la réparation et à la maintenance des équipements sur site. Par exemple, lorsqu’un employé d’un site doit réparer ou entretenir un équipement qui ne lui est pas familier, il peut contacter un technicien plus compétent sur un lieu de travail distant. Il suffit à l’employé d’enfiler un casque et de se laisser guider dans l’opération comme si l’expert travaillait physiquement à ses côtés. L’intervenant distant peut voir ce que voit l’employé sur place et guider celui-ci de manière appropriée. Il peut également dessiner des cercles autour des objets présents dans le champ visuel, ou saisir du texte dans ce dernier pour aider le collaborateur sur place à se concentrer sur les pièces à manipuler. 


Qu’est-ce que l’infrastructure de bureau virtuel (VDI) ?

L’infrastructure de bureau virtuel (VDI) est un environnement de travail qui permet aux employés d’accéder aux applications et aux services de leur entreprise depuis n’importe où. Pour les départements informatiques, elle est devenue le moyen le plus économique de gérer la sécurité et l’infrastructure nécessaires au bon fonctionnement d’une entreprise.
Un datacenter virtuel

Dans cet environnement VDI, les responsables informatiques créent un datacenter virtuel et permettent aux utilisateurs d’accéder aux données, applications et postes de travail de l’entreprise en mode as-a-service via Internet. Tout le traitement est effectué sur un serveur hôte.
Solutions, produits ou services HPE connexes

HPE GreenLake pour la VDI

Solutions VDI

Solutions de stockage VDI pour la virtualisation client
L’accès délocalisé

Les utilisateurs finaux accèdent aux postes de travail virtuels depuis n’importe quel appareil et n’importe quel endroit, mais ils doivent rester connectés au serveur pour pouvoir continuer à travailler. Un « broker » régule ces connexions en agissant comme une passerelle entre les utilisateurs et les serveurs. Cet intermédiaire logiciel vérifie les identifiants et contrôle les accès.

Étant donné que de multiples machines virtuelles peuvent être hébergées sur un même serveur, le nombre de postes de travail virtuels fonctionnant simultanément dans certains environnements VDI peut atteindre plusieurs dizaines de milliers.
Rubriques connexes

Virtualisation de serveurs 

Poste de travail virtuel

Virtualisation

Machine virtuelle (VM)
Comment fonctionne la VDI ?

L’infrastructure de bureau virtuel fonctionne via des machines virtuelles (VM) qui sont contrôlées par un logiciel de gestion centralisé. Dans son principe, une VDI est une configuration de serveur à distance dans laquelle les serveurs sont segmentés en machines virtuelles. Un hyperviseur crée, exécute et gère plusieurs VM hôtes qui contiennent l’environnement de postes de travail virtuels. Ces VM hébergent un système d’exploitation de postes de travail virtuels, et chaque poste de travail virtuel contient une image du système d’exploitation.
Quels sont les avantages de la VDI ?

La VDI offre un certain nombre d’avantages par rapport à un lieu de travail unique.

Le déploiement de la VDI implique que l’entreprise consolide ses ressources sur un serveur hôte. Au fur et à mesure que les utilisateurs sollicitent cette infrastructure, le réseau virtuel permet une utilisation plus efficace de ces ressources. Grâce à cette efficacité accrue, les services informatiques peuvent adapter les besoins en matériel et les achats.

De plus, comme la VDI permet une gestion centralisée, les services informatiques peuvent appliquer des correctifs, des mises à jour et des modifications à l’ensemble des postes de travail virtuels en une seule et même opération. Cela réduit considérablement les efforts nécessaires et simplifie grandement la reprise après sinistre.

L’un des autres avantages majeurs réside dans l’accessibilité inhérente à la VDI. Les utilisateurs peuvent se connecter au moyen de n’importe quel appareil depuis n’importe quel endroit, ce qui facilite réellement l’accès aux fichiers, applications et services de l’entreprise. Cela améliore la productivité et l’expérience utilisateur.

Peut-être plus important encore, les applications et les appareils étant séparés, la VDI permet aux entreprises de verrouiller toutes les données et de maintenir une confidentialité totale. Si un appareil est compromis, le service informatique n’a qu’à le désactiver pour empêcher tout utilisateur non autorisé d’y accéder.

En bref, la transition vers une solution VDI offre de nombreuses possibilités à l’entreprise :

    Maîtriser les coûts
    Maintenir la sécurité
    Assurer une transition transparente entre les espaces de travail
    Garantir une productivité constante
    Optimiser l’utilisation des installations
    Faire monter le matériel en puissance

Comment réussir la mise en œuvre d’une solution VDI ?

Lors de la mise en œuvre d’une solution VDI, il est utile de se concentrer sur le service fourni, et pas uniquement sur les postes de travail eux-mêmes. Afin de garantir une facilité d’utilisation optimale, les utilisateurs doivent savoir et comprendre ce que la VDI va leur apporter.

Par conséquent, la meilleure pratique consiste à démarrer le projet en cherchant à comprendre les besoins des utilisateurs finaux. Certains utilisateurs ont-ils besoin d’applications à forte composante graphique ? Cela nécessitera une configuration particulière. De quel niveau d’efficacité les utilisateurs ont-ils besoin — par exemple, résultats instantanés ou retour d’informations plus mesuré ? Cela aura une incidence sur la puissance de calcul intégrée.

Le réseau lui-même doit également être préparé. Cela signifie qu’il faut évaluer les volumes de trafic réseau en fonction des pics d’utilisation et de l’évolution de la demande, afin de pouvoir estimer la capacité avec plus de précision.

Il est essentiel de s’assurer que chaque poste de travail est provisionné pour gérer le plus haut niveau de ressources qu’il a historiquement consommé. La performance énergétique totale pour le nombre total d’utilisateurs affecte également la capacité à prévoir dans la conception de la VDI.

Enfin, l’exécution d’un pilote pour tester et retester la configuration permettra de s’assurer que la conception fonctionne parfaitement.
Comment les différents secteurs d’activité déploient-ils les solutions VDI ?

En mettant en œuvre des solutions VDI modernes, les entreprises peuvent réduire leurs coûts informatiques, simplifier la gestion des données et des services, et augmenter l’efficacité du stockage et des applications, tout en ajoutant un nombre important de postes de travail à distance. Presque tous les types d’entreprise peuvent bénéficier de l’installation de nouveaux systèmes VDI, mais voici quelques bons exemples :

Santé : Avec l’évolution des pratiques de soin, le secteur de la santé voit augmenter les besoins en capacités multimédias pour la simulation et la téléconsultation en vidéo. Souvent, la solution VDI en place dans les hôpitaux offre des performances graphiques insuffisantes, tandis que les datacenters centralisés et complexes sont gravement compromis en cas d’utilisation intensive. Les solutions VDI actuelles fournissent des réseaux de postes de travail mobiles et efficaces qui allient les avantages d’une informatique centralisée aux performances d’un matériel dédié.

Industrie / Commerce de détail : Les grands fabricants et détaillants mènent des activités sur plusieurs continents avec des milliers d’employés. Les anciennes solutions VDI se sont révélées instables du fait du manque de capacité des serveurs de fichiers, de la faiblesse des sauvegardes et d’une continuité d’activité insuffisante. Les nouvelles infrastructures VDI fonctionnent de manière beaucoup plus fiable et offrent des fonctions de sauvegarde et de restauration à haut débit, ainsi qu’un outil qui automatise les opérations de basculement/rétablissement en cas de sinistre.

Services publics : Les compagnies d’énergie doivent constamment opérer à grande distance, ce qui peut entraîner des problèmes de connectivité, une latence des systèmes et une perte de productivité. Pour surmonter ces problèmes, les fournisseurs d’énergie peuvent réorganiser leurs écosystèmes informatiques avec des solutions VDI qui assurent une disponibilité des données et des applications 24h/24, 7j/7, une évolutivité transparente et une gestion centralisée.

Éducation : Face à l’augmentation considérable de la demande d’enseignement à distance, beaucoup de grands systèmes scolaires ont eu du mal à prendre en charge simultanément un nombre suffisant de postes de travail d’étudiants. En mettant en œuvre une configuration VDI moderne, les établissements scolaires peuvent considérablement augmenter le stockage, la capacité des serveurs et la disponibilité des applications afin que les étudiants puissent utiliser les applications Web de CAO/FAO en technologie flash et les suites d’animation 3D nécessaires à un apprentissage à distance efficace.

• Concept de la virtualisation applicative

Un conteneur Linux, qu'est-ce que c'est ?
Publié 11 mai 2022 •
%t minutes (temps de lecture)
Copier l'URL

Comment migrer depuis CentOS Linux vers un système d'exploitation conçu pour le cloud ?

Pour réussir une migration depuis CentOS Linux vers un système d'exploitation conçu pour le cloud, le choix d’un SE qui corresponde à vos objectifs cloud et adapté à la production est crucial pour la standardisation.
Découvrir la solution

Conteneur Linux : définition

En informatique, un conteneur Linux® est un processus ou un ensemble de processus isolés du reste du système. Tous les fichiers nécessaires à leur exécution sont fournis par une image distincte, ce qui signifie que les conteneurs Linux sont portables et fonctionnent de la même manière dans les environnements de développement, de test et de production. Ainsi, ils sont bien plus rapides à utiliser que les pipelines de développement qui s'appuient sur la réplication d'environnements de test traditionnels. En raison de leur popularité et de leur facilité d'utilisation, les conteneurs constituent également un élément essentiel de la sécurité informatique.
Commencer à utiliser gratuitement des conteneurs Linux
Pourquoi utiliser des conteneurs Linux ?

Imaginons que vous développez une application. Vous travaillez sur un ordinateur portable dont l'environnement présente une configuration spécifique. D'autres développeurs peuvent travailler sur des machines qui présentent des configurations légèrement différentes. L'application que vous développez s'appuie sur cette configuration et utilise des bibliothèques, des dépendances et des fichiers spécifiques. En parallèle, votre entreprise exploite des environnements de développement et de production qui sont standardisés sur la base de configurations qui leur sont propres et de l'ensemble des fichiers associés. Vous souhaitez émuler ces environnements autant que possible localement, mais sans avoir à payer les coûts liés à la recréation des environnements de serveur. Comment faire pour que votre application en développement puisse fonctionner dans ces environnements, passer l'assurance qualité et être déployée sans prise de tête, sans réécriture et sans correctifs ? La réponse est simple : il vous suffit d'utiliser des conteneurs.
What is a container

Le conteneur qui accueille votre application contient l'ensemble des fichiers, des bibliothèques et des dépendances nécessaires. Vous pouvez ainsi le déplacer jusqu'en production sans aucun effet secondaire. Les éléments qui composent l'image d'un conteneur, créés à l'aide d'un outil Open Source comme Buildah, sont analogues à ceux de l'installation d'une distribution Linux, car cette image est fournie avec tous ses paquets RPM, fichiers de configuration, etc. Cependant, il est bien plus simple de distribuer des images de conteneurs que d'installer une nouvelle version d'un système d'exploitation. La crise est évitée, le travail peut continuer.

C'est un exemple simplifié, mais les conteneurs Linux peuvent aussi être utilisés pour résoudre de nombreux problèmes liés à des situations qui nécessitent portabilité, configurabilité et isolation. Les conteneurs Linux permettent de développer plus rapidement des applications et de répondre aux besoins métier dès leur émergence. Dans certains cas, tels que la diffusion de données en temps réel avec Apache Kafka, les conteneurs sont essentiels parce qu'ils constituent le seul moyen de fournir l'évolutivité nécessaire aux applications. Quelle que soit l'infrastructure (sur site, dans le cloud ou hybride), les conteneurs répondent à la demande. Bien sûr, le choix de la plateforme de conteneurs est aussi important que les conteneurs eux-mêmes.

La solution Red Hat® OpenShift® offre tout ce dont vous avez besoin pour le cloud hybride et les conteneurs d'entreprise, mais également pour le développement et les déploiements Kubernetes.
Distribuer des applications conteneurisées avec Red Hat OpenShift
Conteneurs et virtualisation : quelles sont les différences

Ce n'est pas tout à fait la même chose. Les conteneurs et la virtualisation sont plutôt complémentaires. Voici deux définitions simples pour y voir plus clair :

    La virtualisation permet à vos systèmes d'exploitation (Windows ou Linux) de s'exécuter simultanément sur un seul système matériel.
    Les conteneurs partagent le même noyau de système d'exploitation et isolent les processus de l'application du reste du système. Par exemple : les systèmes Linux ARM exécutent des conteneurs Linux ARM, les systèmes Linux x86 exécutent des conteneurs Linux x86 et les systèmes Windows x86 exécutent des conteneurs Windows x86. Les conteneurs Linux sont extrêmement portables, mais ils doivent être compatibles avec le système sous-jacent.

virtualization vs containers

Qu'est-ce que cela signifie ? En premier lieu, la virtualisation utilise un hyperviseur pour émuler le matériel, ce qui permet d'exécuter plusieurs systèmes d'exploitation en parallèle. C'est une solution moins légère que les conteneurs. Lorsque vous disposez de ressources limitées, aux fonctionnalités limitées, il est nécessaire que vos applications soient légères et puissent être déployées de manière dense. Les conteneurs Linux s'exécutent en natif sur leur système d'exploitation, qu'ils partagent entre eux. Vos applications et services restent ainsi légers et s'exécutent rapidement en parallèle.

Les conteneurs Linux représentent une nouvelle évolution de la manière selon laquelle nous développons, déployons et gérons des applications. Les images de conteneurs Linux permettent d'assurer la portabilité et le contrôle des versions des applications. Les développeurs ont ainsi la garantie que ce qui fonctionne sur leur ordinateur portable fonctionnera aussi dans l'environnement de production. Un type spécial d'image de conteneur appelé image d'or crée une référence fiable et cohérente pour la configuration du système. Un conteneur Linux mobilise moins de ressources qu'une machine virtuelle. Il propose une interface standard (démarrage, arrêt, variables d'environnement, etc.), assure l'isolation des applications et peut être géré plus facilement en tant que module d'une application plus importante (plusieurs conteneurs). En outre, ces applications à plusieurs conteneurs peuvent être orchestrées dans différents clouds.

Il existe même des outils qui associent l'orchestration de conteneurs et la gestion de machines virtuelles. Pour plus d'informations, regardez cet extrait du Red Hat Summit 2020 qui comporte une session thématique sur ce type d'outils.
En savoir plus sur la virtualisation
LXC, qu'est-ce que c'est ?

Le projet Linux Containers (LXC) est une plateforme de conteneurs Open Source qui fournit un ensemble d'outils, de modèles, de bibliothèques et de liaisons de langage. Le projet LXC dispose d'une interface en ligne de commande simple qui facilite la prise en main pour les nouveaux utilisateurs.

Il contient un environnement de virtualisation au niveau du système d'exploitation qu'il est possible d'installer sur de nombreux systèmes basés sur Linux. Vous pouvez peut-être y accéder dans le référentiel du package de votre distribution Linux.
Petite histoire des conteneurs

La technologie que nous appelons aujourd'hui « conteneurs » est apparue en 2000 sous le nom de FreeBSD Jails et permettait à l'époque de partitionner un système FreeBSD en plusieurs sous-systèmes ou « jails » (prisons, en français). Les systèmes jails étaient développés comme des environnements sécurisés qu'un administrateur système pouvait partager avec plusieurs utilisateurs opérant à l'intérieur ou à l'extérieur d'une entreprise.

En 2001, Jacques Gélinas a créé le projet VServer, rendant possible la mise en œuvre d'un environnement isolé sur un serveur Linux. Une fois cette base posée pour l'exploitation de plusieurs espaces utilisateurs contrôlés, toutes les pièces se sont mises en place pour former les conteneurs Linux que l'on connaît aujourd'hui.

Très rapidement, d'autres technologies se sont greffées aux conteneurs pour concrétiser cette approche de l'isolement. La fonction du noyau cGroups (groupes de contrôle) permet de contrôler et de limiter l'utilisation des ressources pour un processus ou un groupe de processus. Le système d'initialisation systemd permet de définir l'espace utilisateur et de gérer les processus associés. Il est utilisé par la fonction cgroups pour améliorer le contrôle sur ces processus isolés. Ces deux technologies, qui permettent de mieux contrôler Linux, ont servi de base pour pouvoir exécuter des environnements bien qu'ils soient séparés.
L'avènement de Docker

En 2008, la technologie de conteneurs Docker a fait son apparition (via dotCloud). Cette technologie a apporté de nombreux nouveaux concepts et outils : une interface en ligne de commande simple pour l'exécution et la création de nouvelles images à couches, un démon de serveur, une bibliothèque d'images de conteneurs prédéfinies et le principe d'un serveur de registre. Ensemble, toutes ces technologies ont permis aux utilisateurs de créer rapidement de nouveaux conteneurs à couches et de les partager facilement entre eux.

Trois normes majeures assurent l'interopérabilité des technologies de conteneurs : les spécifications OCI Image, Distribution et Runtime. Ensemble, ces spécifications permettent aux projets communautaires, aux produits commerciaux et aux fournisseurs de cloud de développer des technologies de conteneurs interopérables (qui vous permettent, par exemple, de faire fonctionner vos images créées sur mesure sur le serveur de registre d'un fournisseur de cloud). Aujourd'hui, Red Hat et Docker font partie des nombreux membres de l'Open Container Initiative (OCI) et contribuent à la création de normes ouvertes pour les technologies de conteneurs à l'échelle du secteur.
• 1 TP sur l'installation et la configuration d'un environnement de virtualisation (Virtualbox, Esx, ...)

Docker, qu'est-ce que c'est ?
Mis à jour 11 janvier 2023 •
%t minutes (temps de lecture)
Copier l'URL
Docker : définition

Docker désigne plusieurs éléments, à savoir un projet d'une communauté Open Source, les outils issus de ce projet Open Source, l'entreprise Docker Inc. qui constitue le principal soutien de ce projet, ainsi que les outils que l'entreprise prend officiellement en charge. Le fait que ce terme soit utilisé aussi bien pour désigner les technologies que l'entreprise peut prêter à confusion.

Voici les différentes définitions :

    Le logiciel Docker est une technologie de conteneurisation qui permet la création et l'utilisation de conteneurs Linux®.
    La communauté Open Source Docker a pour but d'améliorer ces technologies en faveur des utilisateurs.
    L'entreprise Docker Inc. s'appuie sur le travail de la communauté Docker pour développer ses produits. En échange, elle y contribue en renforçant la sécurité et en partageant ses avancées. Elle assure ensuite la prise en charge des technologies optimisées et renforcées pour les entreprises clientes.

Grâce à Docker, les conteneurs deviennent des machines virtuelles très légères et modulaires qui vous offrent une grande flexibilité pour créer, déployer, copier des conteneurs et les déplacer d'un environnement à un autre. Vos applications sont ainsi optimisées pour le cloud.
EssayerGérer des conteneurs Docker
Docker et conteneurs Linux : quelle est la différence ?

 
 

 
 

 
 

 
 

Même s'ils sont souvent confondus, Docker et les conteneurs Linux traditionnels sont deux choses bien différentes. À l'origine, la technologie Docker s'appuyait sur la technologie LXC (que beaucoup associent aux conteneurs Linux traditionnels), mais ce n'est plus le cas aujourd'hui. En effet, si LXC offrait une certaine légèreté en matière de virtualisation, elle présentait toutefois des lacunes dans l'expérience de développement et d'utilisation. La technologie Docker permet non seulement de gérer des conteneurs, mais aussi de faciliter leur création et leur conception, ainsi que de distribuer et de contrôler la version des images, entre autres.

Les conteneurs Linux traditionnels utilisent un système d'initialisation capable de gérer plusieurs processus, ce qui permet à plusieurs applications de s'exécuter comme une seule et unique application. La technologie Docker privilégie la division des applications en processus et fournit les outils pour le faire. Cette approche granulaire présente plusieurs avantages.
Avantages des conteneurs Docker
Modularité

Avec Docker, il est possible de séparer un élément d'une l'application afin de le modifier ou de le réparer sans arrêter toute l'application. En plus de cette approche basée sur les microservices, vous pouvez partager des processus entre différentes applications, de la même manière que dans une architecture orientée services (SOA).
Couches et contrôle de version des images

Chaque image Docker se compose de plusieurs séries de couches regroupées dans une seule image. Une nouvelle couche est créée à chaque modification de l'image ou lorsqu'un utilisateur exécute une commande telle que run ou copy.

Docker réutilise ces couches pour construire de nouveaux conteneurs, ce qui permet d'accélérer le processus de création. Les modifications intermédiaires sont partagées entre les images pour améliorer la vitesse, la taille et l'efficacité. Élément indissociable des couches, le contrôle de version permet d'obtenir un journal qui consigne toutes les modifications et vous offre ainsi un contrôle total sur vos images de conteneur.
Restauration

La restauration est sans doute l'un des plus grands avantages des couches. Puisque chaque image est composée de couches, si l'itération actuelle d'une image ne vous plaît pas, vous pouvez restaurer la version précédente. Cette fonction favorise le développement agile et vous aide à mettre en œuvre les pratiques d'intégration et de déploiement continus (CI/CD) au niveau des outils.
Déploiement rapide

Autrefois, l'installation, la mise en marche, le provisionnement et la mise à disposition du matériel prenaient plusieurs jours et impliquaient des efforts et des dépenses importants. Avec les conteneurs Doker, le déploiement s'effectue en quelques secondes seulement. En créant un conteneur pour chaque processus, vous pouvez partager rapidement tous vos processus avec les nouvelles applications. De plus, vous n'avez plus besoin de démarrer un système d'exploitation pour ajouter ou déplacer un conteneur, ce qui réduit considérablement le délai de déploiement. Ainsi, vous pouvez créer et éliminer les données de vos conteneurs facilement, à moindre coût, et sans inquiétude.

La technologie Docker propose donc une approche davantage tournée vers la granularité, le contrôle et les microservices, qui met l'accent sur l'efficacité.

Création d'applications conteneurisées : 5 éléments à partager avec votre hiérarchie
Quelles sont les limites de la technologie Docker ?

Docker est capable de gérer des conteneurs uniques à lui seul. Plus vous utilisez de conteneurs et d'applications conteneurisées et fragmentées, plus la gestion et l'orchestration deviennent complexes. Au bout d'un moment, il est nécessaire de regrouper les conteneurs afin de fournir les services (réseau, sécurité, télémétrie, etc.) sur la totalité d'entre eux. C'est précisément à ce niveau qu'intervient la technologie Kubernetes.
Top 10 des faits que tous les spécialistes du Cloud devraient absolument connaître sur Kubernetes et les conteneurs

 

La technologie Docker ne fournit pas les mêmes fonctionnalités UNIX que les conteneurs Linux traditionnels, notamment la possibilité d'utiliser des processus tels que cron ou syslog dans le conteneur, en parallèle à l'application. Elle ne permet pas non plus le nettoyage des processus petits-enfants une fois le processus enfant terminé. Pour remédier à ces lacunes, vous pouvez modifier le fichier de configuration et y ajouter ces fonctionnalités, mais cela n'est pas toujours évident au premier abord.

De plus, certains sous-systèmes et appareils Linux ne présentent pas d'espaces de noms. Il s'agit entre autres de SELinux, de Cgroups et d'autres appareils /dev/sd*. Cela signifie que si un pirate prenait le contrôle de ces sous-systèmes, l'hôte serait compromis. Afin de préserver sa légèreté, le noyau de l'hôte est partagé avec les conteneurs, ce qui ouvre une brèche de sécurité. Ce n'est pas le cas avec les machines virtuelles, car elles sont bien mieux isolées du système hôte.
Les conteneurs Docker sont-ils vraiment sûrs ?

 

Par ailleurs, le démon Docker peut également poser des problèmes de sécurité. En effet, pour utiliser et exécuter des conteneurs Docker, vous devrez probablement utiliser le démon Docker, un environnement d'exécution persistant pour les conteneurs. Celui-ci nécessite des privilèges root, il est donc important de surveiller l'accès à ces processus et de bien choisir leur emplacement. Par exemple, un démon local présente une surface d'attaque plus petite qu'un démon hébergé dans un emplacement public tel qu'un serveur web.

